---
title: "learnr Tutorial"
author: "Nic Wheeler"
date: "4/12/2019"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Box/ZamanianLab/LabMembers/Nic/R_workshop/")
library(learnr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
```

## Introduction

Welcome to the first installment of the Zamanian Lab R Workshop, created by me, Nic Wheeler. This tutorial will include all of the steps that I typically take in analyzing and visualizing my data: 1) data import, 2) data tidying, 3) data analysis, and 4) data visualization. Along the way, we'll learn a bit about good R syntax, documentation/notation, and data organization. It's my opinion that new languages are best used when a learner is working with their *own* data to solve their *own* problems, so I've designed this tutorial to include selected data that each of you has generated.

In this tutorial we will deal with each of these four steps sequentially and individually, but as you get into more complex analyses you'll find that they can tend to bleed together. That is, sometimes you need to visualize the data before you decide how to analyze it, and other times you need to visualize it to identify outliers or mistakes in data entry, which will lead to changes in your tidying scheme. So, while this tutorial breaks these into discrete steps, it won't always be so straightforward with real life data.

Before we begin, I will first say a bit about the writing and structure of an R script. R code is often **interactive,** which means you will run each line of code in a step-by-step fashion instead of writing an entire script and then running it as a whole. Because of this feature, R is best written in an integrative development environment - an IDE - that includes many helpful coding features. In our case, we're using RStudio as our IDE (if you don't yet have RStudio, please download it [here](https://www.rstudio.com/products/rstudio/download/#download) and install it). Rstudio includes a Source Editor (top-left, where code is written), a Console (bottom-left, where code is run), an Environment (top-right, where all the objects and functions that you've defined are displayed), and a Misc panel (bottom-right, where you can install packages, view files, and view plots). When you begin writing your own code, you'll use RStudio, but for our current purposes we will be using the browser to move through this tutorial.

### Data Import

First things first...import the data. Actually, that's the second thing we have to do. The **real** first thing is to install and load the packages that we'll use for these analyses. Packages are a bunch of R scripts and functions that someone else has written so we don't have to reinvent the wheel. You only have to install a package once, but you have to load it every time you initialize a new R script or environment. You will have to separately install the packages that we need (use the code `install.packages("package name")`), but you'll load them up by running the code below - `library("package name")`. Throughout this tutorial you'll see code blocks like the one below. These are editable, and there will be some exercises where you will be expected to complete the unfinished code so as to make it work. To run the code, click "Run Code." If there was an error during running, click "Start Over," edit the code appropriately, and re-run it. Click "Run Code" to load the several of the packages we'll need along the way.

```{r packages, exercise=TRUE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(readr)
```

For the purposes of this tutorial, we will be loading packages as needed, and I will point out what the package is often used for and what the new function that we're using does. When you begin writing your own R scripts, it's good practice to load all of the packages you'll need at the top of the document, before you start any analysis. If you get halfway through the analysis and realize you need another package, add it to the top.

Ok, now that that's over with, we'll actually import the data. Your data will **almost** always be in the form of a CSV file - a text file with comma-separated values. To import the data, we'll use a function from the `tidyverse` package called `read_csv()` which will read all the data from your CSV file and store it in an objecte called a `data.frame`. Actually, it will load it into a special type of object called a `tibble`, but that's more detail than we need right now. A `data.frame` is basically a set of columns (aka variables) sorted side-by-side. Each column **must** have the same number of rows (aka observations) and **must** contain the same data type (for example, integers, numbers, characters, etc.). Load some of my example data with the code below (`glimpse()` will tell you a little bit about the `tibble` that you just created). 

```{r import, exercise=TRUE}
raw_data <- read_csv("1_rawdata/ce_chemotaxis.csv") %>%
  glimpse()
```

Notice that the `tibble` is assigned a name, `raw_data` using the assignment operator `<-`. Notice also the `%>%` operator. This is called a pipe, and it's something that is specific to the `tidyverse` package. It allows us to sequentially run manipulations on the `tibble` without always assigning it a new name. In this case, it allows us use to run the `glimpse` function, which shows us a little bit about our `tibble`. Normally, you would have to run the command like this: `glimpse(raw_data)`

Use the code block below to import your own data by replacing the `...` with the path and file name of your choice. If you've done it correctly, the `glimpse()` command will show you a bit of your imported data. If not, the command will print an error, something like `'your data' does not exist in current working directory`.

```{r test.import, exercise=TRUE}
test_data <- read_csv("1_rawdata/...") %>%
  glimpse()
```

We now have our data imported to the prefered `tidyverse` object, the `tibble`, and we're ready to tidy, analyze, and visualize our data.

### Data Tidying

There are two ways to structure data - long and wide. Wide data is how most people store data, with observations and variables spread across multiple columns. Wide data is easier to enter manually and often easier for a human to visualize, which is why it persists. However, data analysis in R is better done with long data - where each column is a variable with the same data type and each row is a single observation. I'll demonstrate with the dataset that we imported above, which I generated by running *C. elegans* chemotaxis experiments.

```{r pew, exercise=TRUE}
raw_data <- read_csv("1_rawdata/ce_chemotaxis.csv") %>%
  glimpse()
```

You'll see that `raw_data` is organized such that each observation (row) contains all the data measured from a single chemotaxis plate, and each variable (column) contains either some metadata about the experiment or a recorded observation. Because each row does contains 4 observations, this data is in a wide format and we will need to convert it to long format. But before we do that, we need to first perform an additional calculation.

For this data, we're really interested in a value called the Chemotaxis Index, or CI. The CI is calculated by taking the number of worms found at the cue minus the number of worms found at the control, divided by the total number of worms. While downstream analysis and visualization is best performed on long data, this calculation is most easily performed with the wide data format. The code below will add a new variable, calculate the CI, and add each CI as a new observation.

```{r calculate-setup, echo=FALSE}
raw_data <- read_csv("1_rawdata/ce_chemotaxis.csv")
```

```{r calculate, exercise=TRUE}
library(magrittr)

raw_data %<>% mutate(CI = (Cue_N - Control_N) / Total) %>%
  glimpse()
```

You'll notice a few new things in this code block. First is the new operator `%<>%`, known as the compound assignment operator. This operator was imported via the `magrittr` package, and it's a convenient shorthand for what could also be coded as `raw_data <- raw_data %>% ...`. Second, you'll notice the new function `mutate()`, which is a part of one of the most essential packages of the `tidyverse`, `dplyr`. `dplyr` contains a host of functions that will allow you to "ply" your data (get it? It's a data plier). You can find a handy `dplyr` cheatsheet [here](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf). `mutate()` creates a new column/variable, and it uses observations from other variables (and calculations on those observations) to construct the new column. In this case, we're telling `mutate()` to create a new column named `CI` and to create observations in this column by taking the difference of `Cue_N` and `Control_N` and dividing it by `Total`. `mutate()` will perform this calculation in a vectorized manner, which for simplicity's sake means it will perform it row-by-row. Recall that one of the requirements of the `data.frame` or `tibble` is that each variable must have the same number of observations. This is because of the way that vectorized functions work in the `tidyverse`. If `Cue_N` had more observations than `Control_N`, what would `mutate()` do for the rows with missing data? The function would fail, so keeping your data in a `tibble` will ensure all the downstream vectorized functions will work as expected.

Ok, we now have all the data that we'll need, but it still is in wide format (each row has multiple observations). We need to get it into long format (each row has a single observation) for all the downstream analyses and visualizations. To do that, we use a new `tidyr` function called `pivot_longer()` (Note: depending on when you're going through this tutorial, you may not have the most recent development version of `tidyr`, if the code below results in `could not find function "pivot_longer"`, run `devtools::install_github("tidyverse/tidyr")` and follow the prompts. When it's finished installing, try reloading `tidyverse`, but you may have to close and restart this tutorial). See below:

```{r long-setup, echo=FALSE}
raw_data <- read_csv("1_rawdata/ce_chemotaxis.csv") %>%
  mutate(CI = (Cue_N - Control_N) / Total)
```

```{r long, exercise=TRUE}
data_long <- raw_data %>%
  select(Date, Strain, Cue, Control, Notes, everything()) %>%
  pivot_longer(Cue_N:CI, names_to = "Measurement", values_to = "Value") %>%
  glimpse()
```

We first had to order the data in a tidier fashion by ordering all the metadata (the date, strain, cue, control, and notes) before the actual observational data. Then we made the data long. The `pivot_longer()` function included a number of arguments. First, we told it which variables to convert to long format, in this case all the columns from `Cue_N` to `CI`, because each of these variables contain the same data type (a number) and a single observation. Second, we told it to move the column names to a new variable called `Measurement`, and we told it to move all the observations below the column names to a new variable called `Value`. The `pivot_longer()` function will take every observation in the `data.frame`, move it to its own individual row, copy in the metadata that we told it not to pivot, and add a new variable next to it that corresponds to the column it came from. (Yes, this is confusing, it might take some staring at the before and after `tibbles` to figure out exactly what's going on.) The resulting `tibble` has 7 columns/variables - `Date`, `Strain`, `Cue`, `Control`, `Notes`, `Measurement`, and `Value` - in long format. Each row contains a single observation - the total number of N2 worms on one plate from 2018-12-17, for instance - and each column contains values of the same class - `Date` and `Count` contain numbers (the "double" class, to be specific), and the others contains character classes (i.e. words). This is the sort of format that works well with downstream analysis and plotting. See [this](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) reference for more information on tidy data.

Before we move on to analysis, I want to say a bit here about object naming. So far, we have two objects - `raw_data` and `data_long`. In a given project you may end up with hundreds of different objects, each with a unique name, and you will want to give names that still make sense a year from now when you open up your script for the first time in months, when you send the script to a collaborator, or when you publish the script with a corrresponding paper. You can imagine coming up with appropriate object names might become difficult. Here are a few hints that may help:

1. Names should be as short as possible (i.e. `temp`, not `temporary`).
2. At the same time, names should be as descriptive as possible. Almost all of my scripts start with `raw_data`, `data_long`, and `tidy_data`. I know exactly what each of these objects will contain even before starting to run the script.
3. For technical reasons, avoid dot-script (`tidy.data`), but feel free to use snake-script (`tidy_data`) or camel-script `tidyData`.

Now that we know the difference between wide and long data, take a look at your own data that you imported earlier. Is it wide or long? If it's wide, use `pivot_longer()` in the code block below. If it's already long, reassign it to a new object with `_long` included in the new object name. You will have to comment out the lines below that you don't want to run by adding `#` to the beginning.

```{r test.long, exercise=TRUE}
test_data <- read_csv("1_rawdata/...")

# convert from wide to long data
data_long <- test_data %>%
  select(...) %>%
  pivot_longer(...)

# or, simply re-assign the data to a more descriptive name
data_long <- test_data
```

Our data is now in a tidy, long format and ready to be analyzed! Before I begin analysis, I like to save my tidy data so that if I close this script but then want to come back later, I don't have to re-do all the tidying. In this test case, it wouldn't be a big deal to just rerun the import and tidying commands, but often we will run into large datasets were the import and tidying will take several minutes to complete. Because these two steps will not change later, even if I need to change the downstream analysis, I can save the tidy data in an intermedate file for later import.

I could save the data as a CSV file, and that would be a good idea if I wanted to view the data as a spreadsheet or in a text editor. Usually that isn't the case, so R provides a binary storage format that takes up a miniscule amound of hard drive space and is very quickly imported into R - an RDS file. We'll save our tidied `tibble` using the code below:

```{r save-setup, echo=FALSE}
raw_data <- read_csv("1_rawdata/ce_chemotaxis.csv")

data_long <- raw_data %>%
  mutate(CI = (Cue_N - Control_N) / Total) %>%
  select(Date, Strain, Cue, Control, Notes, everything()) %>%
  pivot_longer(Cue_N:CI, names_to = "Measurement", values_to = "Value") 
```

```{r save, exercise=TRUE}
tidy_data <- data_long
saveRDS(tidy_data, "2_tidydata/tidy_data.rds")
```

Notice that I first renamed the long data to `tidy_data`. In reality this was unnecessary; I could have saved `data_long` directly. However, with real life data, there might be more tidying that needs to happen after getting the data in long format, and only the completed tidy dataset should be saved as an RDS file.

You'll also notice that the path to the folder used for saving has changed. In the import section, we read the CSV file from a folder called `1_rawdata`. Now that it's tidied, we're saving it to a folder called `2_tidydata`. This is a helpful convention that I urge you to use to help organize your data. I recommend having at least 4 separate folders within each distinct project:

1. `1_rawdata`
2. `2_tidydata`
3. `3_completedata`
4. `4_plots`

This structure will help organize the analysis from start to finish, and it will help others who may have never seen the data structure before understand what is going on when they view the folders.

Save your own tidied data with the code block below:

```{r test.save, exercise=TRUE}
test_data <- read_csv("1_rawdata/...")

# convert from wide to long data
data_long <- test_data %>%
  select(...) %>%
  pivot_longer(...)

# or, simply re-assign the data to a more descriptive name
data_long <- test_data

tidy_data <- data_long

saveRDS(tidy_data, "2_tidydata/...")
```

### Data Analysis

The next step is data analysis, which includes all of the manipulations required to filter or select the data you want, calculate new variables, and calculate summary statistics like mean, median, and standard deviation. `dplyr` will contain almost all of the functions required to perform these manipulations.

Let's get back to my chemotaxis dataset. When I was first running these experiments, I was using 6 cm NGM plates. I later discovered that this was **not** the correct plate size or agar formulation, so I started using 10 cm plates with chemotaxis agar. Instead of tossing out all the older data entirely, I kept it in the same CSV and encoded the `Notes` variable to include these differences. In the downstream analysis, we only want to analyze observations that came from a 10 cm plate, and we can do that with `dpylr`'s `filter()` function. See below:

```{r filter-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")
```

```{r filter, exercise=TRUE}
data_filt <- filter(tidy_data, Notes == "10 cm CTX") %>%
  glimpse()
```

Check the output. You'll notice the same variables remain, but there are fewer observations, only those that satisfy the filter that we setup. Go back to the `dplyr` cheatsheet that I provided earlier. You'll see our `filter()` function in the Subset Observations section. In the lower Logic in R table you'll find a number of different logical tests that `filter()` can use. In our case, we used `==` to only choose `Notes` observations that were equal to "10 cm CTX". If you look closely, there is an additional way to use the `filter()` function and achieve the same output that we achieved above. Use the code block below to try to find one of these `filter()` commands (replace `...` with your code).

```{r filter.test-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")
```

```{r filter.test, exercise=TRUE}
temp_data_filt <- filter(tidy_data, ...) %>%
  glimpse()
```

If your filtered `tibble` has 450 observations, then you've found the correct alternative.

Now that we have a filtered dataset, we want to take a quick look at the mean CI for each strain and test cue. To do this, we first group the tidied data into the groups that we want to compare, then we summarize it into a new `tibble` containing our summary statistics. See below:

```{r summary-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")
```

```{r summary, exercise=TRUE}
data_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value))) %>%
  glimpse()
```

As stated above, we first used the `group_by()` function to group our filtered data by Strain, Cue, and Measurment. Next, we used the `summarize()` function to create three new variables from each of the defined groups. R has  built-in functions for calculating the mean and standard deviation, but we had to manually calculate the standard error of the mean. You'll notice that the resulting `tibble` has much fewer observations - one for every group. Check out the `dplyr` cheatsheet for other useful summary functions.

In reality, we probably aren't interested in the "Cue_N", "Control_N", "Outside", and "Total", because we'll never actually plot this data. We're only interested in "CI". Use the code block below and one of the functions we learned earlier to create a new summary `tibble` that only contains observations of "CI".

```{r ci-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")
```

```{r ci, exercise=TRUE}
ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  ...() %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value))) %>%
  glimpse()
```

If you did it correctly, `ci_summ` will have 6 observations and 6 variables.

In this summary, we did not group by the experiment date, so  variation that we see through the standard deviation and standard error of the mean is a measure of biological variation across all the experiments that I ran. We might also be interested in the technical variation between individual experiments (this data was collated from 3 independent biological replicates for each cue, and each biological replicate included 5 technical replicates per strain.) To get a measure of technical variation, you will have to change the grouping strategy. Use the code block below to get this summary data.

```{r tech-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")
```

```{r tech, exercise=TRUE}
tech_summ <- group_by(data_filt, ...) %>%
  ...() %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value))) %>%
  glimpse()
```

The successful result of `tech_summ` will have 18 observations and 7 variables.

### Data Visualization

We now have a measure of variation and possible batch effects, which is caused by technical variation during the experiment and will be found across the entire batch; one example of a batch effects is an increase in mean CI for all strains in comparison to previous experiments because of an increased lab temperature on that day, or because of a misdilution of the cue. To more reliably examine for batch effects, we need to plot the data. For all of our plotting, we'll use the `tidyverse` package `ggplot2`, which enables us to very simply modulate almost every imaginable visualization parameter.

A plot is initialized with the `ggplot()` function, and we use arguments in the function to tell it about what data to plot and the aesthetics of the plot (for instance, which variables should be plotted on the x and y axis). After initialization, we can add `geom` layers with the `+` operator. This allows us to very quickly cycle through a number of different plotting stratgies by simply adjusting the type of layer that we add. Below is a demonstration.

```{r plot-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

tech_summ <- group_by(data_filt, Date, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot, exercise=TRUE}
tech_plot <- ggplot(tech_summ, aes(x = Strain, y = Mean)) +
  geom_point()
tech_plot
```

We told `ggplot()` that we wanted to plot the `tech_summ` data, and that the x-axis should include the different strains while the y-axis should include the means (in this case the mean CI). `ggplot()` is able to intuit the order of the axes and the plotting range, and it then plots points with `geom_point()`. If you examine the resulting plot, you may see a few issues. First, N2 has twice as many points as the others. If you look back at `tidy_data`, you'll remember that N2 was used as a positive control for two sets of experiments, one using isoamyl alcohol as a cue, and another using diacetyl as a cue. This means it really isn't appropriate to plot all of the same data on the same plot, so we'll have to separate those datasets in some way. The scond issues is that it looks like both CX10 and ZAM14 only have 2 points plotted, but the data in `tech_summary` has 3 CI observations for both of these strains. If you look closely at the data, you'll see that for each of these strains, two of the observations are very near each other, which suggests that maybe in the plot these points are overlapping and obscurring each other. We'll deal with both of these issues below.

```{r plot2-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

tech_summ <- group_by(data_filt, Date, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot2, exercise=TRUE}
tech_plot <- ggplot(tech_summ, aes(x = Strain, y = Mean)) +
  # geom_point() +
  geom_jitter(aes(color = Date)) +
  facet_grid(~Cue, scales = "free_x") +
  NULL
tech_plot
```

For this plot, we removed the `geom_point()` layer and instead added a `geom_jitter()` layer. These two functions use the same geoms (i.e. a point), but `geom_jitter()` will jitter the points to prevent overlaps. We also added a `facet_grid()` command, which the first argument being an expression that told the function how to split the plot grid, either into rows or into columns. In this case, we wanted the grid split into columns. The second argument let the function know that it didn't have to include every possible value on the x-axis, only the values that had observations that would be plotted in that facet. Finally, we added a new color aesthetic to the `geom_jitter()` layer. We could have defined this aesthetic in original `ggplot()` function, but that aesthetic will then be passed to every single layer. This is problematic if we add new layers but want to color by different variables, so it's better to add color aesthetics layer by layer. The other new thing is the addition of `NULL` after a final `+` operator. This is a practice that I have to enable faster debugging of plotting code. When quickly changing layers and aesthetics, you will find that you often end up commenting out lines instead of removing them entirely. If you comment out the final line, your new last line will contain a `+` operator at the end that will cause the entire command to fail. IF you add NULL to the last line, this problem will never happen.

A problem still remains with this plot, though. You can see that even though all the diacetyl replicates were performed on different days, the range of colors used makes it look like they were performed on the same day. Recall that in our original `tibble`, the Date variable was interpreted as a numeric class, which means `ggplot()` isn't seeing this variable as a true data, but as a number. To change the way the colors are assigned so that different days have more distinguishable colors, we need to give the Date variable a new class. We could go back to the original `tibble` and reassign it, or we can tell `ggplot()` to interpret the variable as a different class before plotting. We will do the latter below.

```{r plot3-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

tech_summ <- group_by(data_filt, Date, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot3, exercise=TRUE}
tech_plot <- ggplot(tech_summ, aes(x = Strain, y = Mean)) +
  # geom_point() +
  geom_jitter(aes(color = as.factor(Date))) +
  facet_grid(~Cue, scales = "free_x") +
  NULL
tech_plot
```

The plot is now at a point where we can begin to assess the sources of variation. The experiment dates are now interpreted as factors, which can be thought of as a categorical variable with no inherent arrangment (as opposed to numeric variables which are continuous and arrangment from the least to the greatest).

Let's now look at the variation. One thing that is immediately apparent is that the PR678 and ZAM14 mean CI's were much higher on 20190316 than the other two days, but that the N2 CI wasn't. It's hard to guess why this occurred, but it it is something to remember as we continue to plotting the actual observations. 

This plot tells us that the data is trustworthy and doesn't include extreme outliers that will skew the remaining analysis and visualization. We can now feel comfortable plotting the original filtered dataset.

```{r plot4-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")
```

```{r plot4, exercise=TRUE}
ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  geom_jitter() +
  facet_grid(~Cue, scales = "free_x") +
  NULL
ci_plot
```

Remember, we're only interested in plotting the CI. We could have filtered `data_filt` to only include "CI" as a `Measurement` and assigned it a new name, but that is probably unnecessary. Instead, we can do the filter directly within the `ggplot()` function, which doesn't change the underlying `data_filt` `tibble` but only plots the observations that pass the filter.

As you can see, all the observations successfully plotted. However, there are again several problems that preclude drawing inferences from the visualization. First, the jittered points run between strains and it's not clear where one strain starts and the other begins. Second, the labels on the y-axis are pretty sparse. Third, the error that we calculated in the data analysis section isn't included at all. Finally, the y-axis label "Value" is meaningless to someone who hasn't looked at the data itself. Let's fix those problems.

```{r plot5-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot5, exercise=TRUE}
library(ggbeeswarm)

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(color = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL
ci_plot
```

We're nearly there! Instead of randomly jittered points we used a new function from the package `ggbeeswarm` to arrange the points in more predictable fashion. We also added colors to the make differentiation of the strains absolutely clear. Second, we added a new layer, `geom_pointrange()` which plots the mean and standard errors that we calculated in `ci_summ` (Note: `geom_pointrange()` requires three aesthetics - y, ymin, and ymax. To calculate these we passed the `ci_summ` data instead of `data_filt` which was passed in `ggplot()`). Next, we modified the y-axis with the `scale_y_continuous()` function, which accepted the `limits` and `breaks` arguments. The `limits` are self-explanatory, but the solution for the `breaks` argument - where the labels and ticks will occur, is a bit more complex. Here, we used the `seq()` function to create a vector of numbers from -0.5 to 1, with breaks every 0.25. If you run `seq(from = -0.5, to = 1, by = 0.25)` alone you can see the output, and if you run `?seq()` you can see the documentation for this function. In fact, you can read the documentation of every single function that is loaded by running the function with a `?` prepeneded, which is how I knew about the three different aesthetics required by `geom_pointrange`. We next used `labs()` to redefine the y-axis label, and `theme()` to remove the legend. `theme()` is an incredibly powerful function that allows you to customize almost every imaginable feature of the plot. We'll get to a longer discusion on `theme()` shortly.

There's one final thing to fix, and that is the order of the strains on the x-axis. Unfortunately, there is no good way to do that within the plotting command itself, and we'll have to go back to filtered `tibble`.

```{r plot6-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot6, exercise=TRUE}
data_filt %<>% mutate(Strain = factor(Strain, levels = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")))

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(color = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL
ci_plot
```

To reorder the `Strain` variable, we had to change it from a character class to a factor class, as we did with `Date` earlier. Instaed of doing it in the plot command, we added a "new" variable (in reality we replaced the old `Strain` variable with a new, factored `Strain` variable) with the `mutate()` function, which was instructed to make the variable a factor. Additionally, we used the `levels` argument to assign a predefined arrangement, which would then be incorporated in the plot.

Congratulations! You have a nearly publication quality data visualization. At this point, there are several next steps. First, you can continue probing the data by adding new filters or changing some of the colors. Remember the possible batch effect we found in two strains? You could change the color aesthetic to get a more granular view of the technical variation. In fact, use the below code block to do that (again, replace the `...`).

```{r plot7-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))
```

```{r plot7, exercise=TRUE}
data_filt %<>% mutate(Strain = factor(Strain, levels = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")))

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(... = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL
ci_plot
```

The last two tasks are what it will take to move your plots from being merely "in-house" quality to being "publication" quality.

#### Theming {data-progressive=TRUE}

Now that you know a bit about plotting with `ggplot2`, you'll begin noticing "ggplots" **everywhere**. One goal of the plots coming out of the Zamanian Lab is that we don't want them to look "stock ggplot." One easy way to do that is by using one of the complete `ggplot2` [themes](https://ggplot2.tidyverse.org/reference/ggtheme.html). Here are a few examples:

```{r theme-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))

data_filt %<>% mutate(Strain = factor(Strain, levels = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")))

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(color = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL
```

```{r theme, exercise=TRUE}
library(cowplot)

bw <- ci_plot + theme_bw()

dark <- ci_plot + theme_dark()

minimal <- ci_plot + theme_minimal()

classic <- ci_plot + theme_classic()

plot_grid(bw, dark, minimal, classic)
```

Notice first that adding the complete theme removed the theme customizations that we had previously defined (namely, the legend is back). I also used the new function `plot_grid` from the package `cowplot` to plot all four plots in a grid.

Complete themes are important, and they're often a good starting point, but any good plot will need to be specifically customized for the eventual place of publication. In the below code I incorporate a number of new thematic elements that can act as a starting point for your own theming. All of the theme customizations can be found at `?theme()` or [here](https://ggplot2.tidyverse.org/reference/theme.html).

```{r theme2-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))

data_filt %<>% mutate(Strain = factor(Strain, levels = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")))

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(color = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL
```

```{r theme2, exercise=TRUE}
ci_plot <- ci_plot +
  geom_hline(aes(yintercept = 0), linetype = "dashed", colour = "grey37", size = 0.5) +
  scale_color_viridis_d() +
  theme_minimal(base_size = 16, base_family = "Helvetica") +
  theme(
    axis.text.x = element_text(face = "bold.italic", size = 10, angle = 45, vjust = .5),
    axis.text.y = element_text(face = "bold", size = 9),
    axis.title.x = element_text(angle = 90, vjust = 0.5, size = 0),
    axis.title.y = element_text(face = "bold", angle = 90, size = 12),
    strip.text.x = element_text(face = "bold", size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(size = 0.75, colour = "black"),
    legend.position = "none"
  ) +
  NULL
ci_plot
```

You can see that I first added a new layer and changed the color scale. Then I added with a complete theme and ascribed a `base_size` and `base_family` to standardize the textual elements. The rest of the modifications came as arguments to the `theme()` function.

### Statistics Layers {data-progressive=TRUE}

After completing the theme, we need to add symbols denoting statistical comparisons, and then we'll be finished. Fortunately, it will only take a few additional lines of code to do so.

```{r stats-setup, echo=FALSE}
tidy_data <- readRDS("2_tidydata/tidy_data.RDS")

data_filt <- filter(tidy_data, Notes == "10 cm CTX")

ci_summ <- group_by(data_filt, Strain, Cue, Measurement) %>%
  filter(Measurement == "CI") %>%
  summarize(Mean = mean(Value), SD = sd(Value), SEM = sd(Value) / sqrt(length(Value)))

data_filt %<>% mutate(Strain = factor(Strain, levels = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")))

ci_plot <- ggplot(filter(data_filt, Measurement == "CI"), aes(x = Strain, y = Value)) +
  # geom_point() +
  # geom_jitter() +
  geom_beeswarm(aes(color = Strain)) +
  geom_pointrange(data = ci_summ, aes(y = Mean, ymin = Mean - SEM, ymax = Mean + SEM)) +
  # scale_x_discrete(breaks = c("N2", "CX10", "PR678", "ZAM13", "ZAM14")) +
  scale_y_continuous(limits = c(-0.5, 1), breaks = seq(from = -0.5, to = 1, by = 0.25)) +
  facet_grid(~Cue, scales = "free_x") +
  labs(y = "Chemotaxis Index (CI)") +
  theme(legend.position = "none") +
  NULL

ci_plot <- ci_plot +
  geom_hline(aes(yintercept = 0), linetype = "dashed", colour = "grey37", size = 0.5) +
  scale_color_viridis_d() +
  theme_minimal(base_size = 16, base_family = "Helvetica") +
  theme(
    axis.text.x = element_text(face = "bold.italic", size = 10, angle = 45, vjust = .5),
    axis.text.y = element_text(face = "bold", size = 9),
    axis.title.x = element_text(angle = 90, vjust = 0.5, size = 0),
    axis.title.y = element_text(face = "bold", angle = 90, size = 12),
    strip.text.x = element_text(face = "bold", size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.background = element_blank(),
    axis.line = element_line(size = 0.75, colour = "black"),
    legend.position = "none"
  ) +
  NULL
```

```{r stats, exercise=TRUE}
library(ggpubr)

diacetyl.comparisons <- list(c("N2", "CX10"), c("N2", "ZAM13"), c("CX10", "ZAM13"))
ia.comparisons <- list(c("N2", "PR678"), c("N2", "ZAM14"), c("ZAM14", "PR678"))

ci_plot <- ci_plot +
  stat_compare_means(comparisons = diacetyl.comparisons, method = "t.test", label = "p.signif", label.y = c(1.05, 1.2, 1.35)) +
  stat_compare_means(comparison = ia.comparisons, method = "t.test", label = "p.signif", label.y = c(1.05, 1.2, 1.35)) +
  scale_y_continuous(limits = c(-0.5, 1.4), breaks = seq(from = -0.5, to = 1, by = 0.25))
ci_plot
```

`ggpubr` is a great package that enables easy addition of statistical tests to your plot. To do so, you first have to make a list of vectors that defines the pairwise comparisons to perform, and pass this list to the `comparisons` argument of `stat_compare_means()`. The other arguments define the statistical test used, what labels are added, and where those labels are plotted. In order to make room for these lables, we had to redefine the y scale to extend the limits.







